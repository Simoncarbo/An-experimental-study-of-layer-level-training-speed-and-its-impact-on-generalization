{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sicarbonnell/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from optimizers_llc import get_optimizer\n",
    "from experiment_utils import import_cifar,history_todict, lr_schedule\n",
    "from rotation_rate_utils import get_kernel_layer_names, LayerwiseParameterDistanceMemory\n",
    "from models import VGG_pytorchBlog\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    if not os.path.isfile('adaptive_gradients.p'):\n",
    "        return {}\n",
    "    else:\n",
    "        with open('adaptive_gradients.p','rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump_results(results):\n",
    "    with open('adaptive_gradients.p','wb') as f:\n",
    "        pickle.dump(dict(results),f)\n",
    "\n",
    "def update_results(path, key, value):\n",
    "    results = load_results()\n",
    "    position = results\n",
    "    for p in path:\n",
    "        position = position[p]\n",
    "    position.update({key:value})\n",
    "    dump_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results = True\n",
    "if not save_results:\n",
    "    results = {}\n",
    "monitor_file = 'monitor.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = import_cifar()\n",
    "\n",
    "model = VGG_pytorchBlog()\n",
    "layer_names = get_kernel_layer_names(model)\n",
    "initial_kernels = list(zip(layer_names,[model.get_layer(l).get_weights()[0] for l in layer_names]))\n",
    "\n",
    "for optimizer in ['SGD', 'RMSprop', 'SGD_AMom', 'Adam', 'Adagrad']:\n",
    "\n",
    "    if save_results:\n",
    "        results = load_results()\n",
    "        if optimizer not in results.keys():\n",
    "            update_results([],optimizer,{})\n",
    "    elif optimizer not in results.keys():\n",
    "        results.update({optimizer:{}})\n",
    "\n",
    "    for training_mode in ['llc', 'normal']:\n",
    "        start = time.time()\n",
    "        if training_mode == 'llc':\n",
    "            model = VGG_pytorchBlog(weight_decay = 0.) # for some (numerical?) reason, weight decay made Layca fail\n",
    "        else:\n",
    "            model = VGG_pytorchBlog()\n",
    "\n",
    "        batch_size = 128\n",
    "        verbose = 0\n",
    "        if training_mode == 'normal':\n",
    "            epochs = 250\n",
    "            lr_bib = {'SGD':0.5, 'RMSprop':0.0003,'SGD_AMom':0.5,'Adam':0.0003,'Adagrad':0.01 }\n",
    "            lr = lr_bib[optimizer]\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule(lr,0.5,[i*25 for i in range(1,100)]))\n",
    "        elif training_mode == 'llc':\n",
    "            epochs = 250\n",
    "            lr_bib = {'SGD':3**-3, 'RMSprop':3**-3,'SGD_AMom':3**-5,'Adam':3**-5,'Adagrad':3**-3 }\n",
    "            lr = lr_bib[optimizer]\n",
    "            lr_scheduler = LearningRateScheduler(lr_schedule(lr,0.2,[100,170,220]))\n",
    "        \n",
    "        batch_frequency = int((x_train.shape[0]/batch_size))+5 # once per epoch\n",
    "        lpdm = LayerwiseParameterDistanceMemory(initial_kernels, batch_frequency = batch_frequency)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=get_optimizer(optimizer, training_mode, model, lr),\n",
    "                      metrics=['accuracy', 'categorical_crossentropy'])\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "            datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "            history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                                          steps_per_epoch=int(x_train.shape[0]/batch_size),\n",
    "                                          epochs=epochs,\n",
    "                                          validation_data=(x_test,y_test),\n",
    "                                          verbose = verbose,\n",
    "                                          callbacks = [lr_scheduler, lpdm])\n",
    "\n",
    "        if save_results:\n",
    "            update_results([optimizer],training_mode,{'history':history_todict(history),'lpdm':np.array(lpdm.memory)})\n",
    "        else:\n",
    "            results[optimizer].update({training_mode:{'history':history_todict(history),'lpdm':np.array(lpdm.memory)}})\n",
    "\n",
    "        with open(monitor_file,'a') as file:\n",
    "            file.write(optimizer+', '+training_mode+': done in '+str(time.time()-start)+' seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
