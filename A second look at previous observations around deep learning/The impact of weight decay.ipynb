{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sicarbonnell/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from optimizers_llc import get_optimizer\n",
    "from experiment_utils import import_cifar,history_todict, lr_schedule\n",
    "from rotation_rate_utils import get_kernel_layer_names, LayerwiseParameterDistanceMemory\n",
    "from models import VGG_pytorchBlog\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    if not os.path.isfile('impact_of_weight_decay.p'):\n",
    "        return {}\n",
    "    else:\n",
    "        with open('impact_of_weight_decay.p','rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump_results(results):\n",
    "    with open('impact_of_weight_decay.p','wb') as f:\n",
    "        pickle.dump(dict(results),f)\n",
    "\n",
    "def update_results(path, key, value):\n",
    "    results = load_results()\n",
    "    position = results\n",
    "    for p in path:\n",
    "        position = position[p]\n",
    "    position.update({key:value})\n",
    "    dump_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results = True\n",
    "if not save_results:\n",
    "    results = {}\n",
    "monitor_file = 'monitor_weight_decay.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = import_cifar()\n",
    "\n",
    "model = VGG_pytorchBlog()\n",
    "layer_names = get_kernel_layer_names(model)\n",
    "initial_kernels = list(zip(layer_names,[model.get_layer(l).get_weights()[0] for l in layer_names]))\n",
    "\n",
    "for optimizer in ['SGD']:\n",
    "\n",
    "    if save_results:\n",
    "        results = load_results()\n",
    "        if optimizer not in results.keys():\n",
    "            update_results([],optimizer,{})\n",
    "    elif optimizer not in results.keys():\n",
    "        results.update({optimizer:{}})\n",
    "\n",
    "    for training_mode in ['normal']:\n",
    "        if save_results:\n",
    "            results = load_results()\n",
    "            if training_mode not in results[optimizer].keys():\n",
    "                update_results([optimizer],training_mode,{})\n",
    "        elif training_mode not in results[optimizer].keys():\n",
    "            results[optimizer].update({training_mode:{}})\n",
    "        \n",
    "        for weight_decay in [False, True]:\n",
    "        \n",
    "            start = time.time()\n",
    "            if weight_decay:\n",
    "                model = VGG_pytorchBlog()\n",
    "            else:\n",
    "                model = VGG_pytorchBlog(weight_decay = 0.)\n",
    "\n",
    "            batch_size = 128\n",
    "            verbose = 0\n",
    "            if training_mode == 'normal':\n",
    "                epochs = 250\n",
    "                lr_bib = {'SGD':0.5, 'RMSprop':0.0003,'SGD_AMom':0.5,'Adam':0.0003,'Adagrad':0.01 }\n",
    "                lr = lr_bib[optimizer]\n",
    "                lr_scheduler = LearningRateScheduler(lr_schedule(lr,0.5,[i*25 for i in range(1,100)]))\n",
    "            elif training_mode == 'llc':\n",
    "                epochs = 250\n",
    "                lr_bib = {'SGD':3**-3, 'RMSprop':3**-3,'SGD_AMom':3**-5,'Adam':3**-5,'Adagrad':3**-3 }\n",
    "                lr = lr_bib[optimizer]\n",
    "                lr_scheduler = LearningRateScheduler(lr_schedule(lr,0.2,[100,170,220]))\n",
    "\n",
    "            batch_frequency = int((x_train.shape[0]/batch_size))+5 # once per epoch\n",
    "            lpdm = LayerwiseParameterDistanceMemory(initial_kernels, batch_frequency = batch_frequency)\n",
    "\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=get_optimizer(optimizer, training_mode, model, lr),\n",
    "                          metrics=['accuracy', 'categorical_crossentropy'])\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "                datagen = ImageDataGenerator(horizontal_flip=True)\n",
    "\n",
    "                history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                                              steps_per_epoch=int(x_train.shape[0]/batch_size),\n",
    "                                              epochs=epochs,\n",
    "                                              validation_data=(x_test,y_test),\n",
    "                                              verbose = verbose,\n",
    "                                              callbacks = [lr_scheduler, lpdm])\n",
    "\n",
    "            if save_results:\n",
    "                update_results([optimizer,training_mode],weight_decay,{'history':history_todict(history),'lpdm':np.array(lpdm.memory)})\n",
    "            else:\n",
    "                results[optimizer][training_mode].update({weight_decay:{'history':history_todict(history),'lpdm':np.array(lpdm.memory)}})\n",
    "\n",
    "            with open(monitor_file,'a') as file:\n",
    "                file.write(optimizer+', '+training_mode+', '+str(weight_decay)+': done in '+str(time.time()-start)+' seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
