{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script used for training on the two s.o.t. tasks of the paper (2 last tasks of Table 1):  \n",
    "- VGG model on cifar10 from pytorchblog: http://torch.ch/blog/2015/07/30/cifar.html\n",
    "- WRN 28 - 10 with dropout 0.3 trained on cifar100\n",
    "\n",
    "Parameters that can be looped on:\n",
    "- task solved\n",
    "- optimization method used (SGD, adaptive gradient methods, +_layca, +_weight_decay)\n",
    "\n",
    "The results (training history, layer-wise angle deviation curves, test  performance) are saved in a dictionary.\n",
    "A result can be easily found through: results[task][optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "\n",
    "import math as m\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from experiment_utils import history_todict, get_val_split\n",
    "from rotation_rate_utils import LayerwiseAngleDeviationCurves\n",
    "\n",
    "from layca_optimizers import SGD\n",
    "\n",
    "from import_task import import_task\n",
    "from get_training_utils import get_training_schedule_sot, get_optimizer, get_learning_rate_multipliers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities for storing the results in pickle files\n",
    "def load_results():\n",
    "    if not os.path.isfile('results.p'):\n",
    "        return {}\n",
    "    else:\n",
    "        with open('results.p','rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump_results(results):\n",
    "    with open('results.p','wb') as f:\n",
    "        pickle.dump(dict(results),f)\n",
    "\n",
    "def update_results(path, new_data):\n",
    "    results = load_results()\n",
    "    position = results\n",
    "    for p in path:\n",
    "        position = position[p]\n",
    "    # new_data is a dictionary with the new (key,value) pairs\n",
    "    position.update(new_data)\n",
    "    dump_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if results should be saved in the file or not\n",
    "save_results = True\n",
    "if not save_results:\n",
    "    results = {}\n",
    "# file for monitoring the experiment's progress\n",
    "monitor_file = 'monitor_experiment.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "390/390 [==============================] - 191s 491ms/step - loss: 7.1594 - acc: 0.1227 - val_loss: 6.0320 - val_acc: 0.1325\n",
      "Epoch 2/200\n",
      "390/390 [==============================] - 189s 485ms/step - loss: 4.6483 - acc: 0.2680 - val_loss: 4.2725 - val_acc: 0.2694\n",
      "Epoch 3/200\n",
      "390/390 [==============================] - 189s 486ms/step - loss: 3.5342 - acc: 0.3608 - val_loss: 3.9556 - val_acc: 0.2834\n",
      "Epoch 4/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 3.0062 - acc: 0.4194 - val_loss: 3.5397 - val_acc: 0.3339\n",
      "Epoch 5/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.7507 - acc: 0.4620 - val_loss: 3.4395 - val_acc: 0.3356\n",
      "Epoch 6/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.6071 - acc: 0.4929 - val_loss: 3.4528 - val_acc: 0.3385\n",
      "Epoch 7/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5301 - acc: 0.5163 - val_loss: 3.6861 - val_acc: 0.3425\n",
      "Epoch 8/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4839 - acc: 0.5351 - val_loss: 4.0263 - val_acc: 0.2591\n",
      "Epoch 9/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4677 - acc: 0.5462 - val_loss: 3.0657 - val_acc: 0.4302\n",
      "Epoch 10/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4435 - acc: 0.5600 - val_loss: 3.2514 - val_acc: 0.4096\n",
      "Epoch 11/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4257 - acc: 0.5671 - val_loss: 2.8907 - val_acc: 0.4793\n",
      "Epoch 12/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4174 - acc: 0.5812 - val_loss: 3.6640 - val_acc: 0.3787\n",
      "Epoch 13/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4144 - acc: 0.5852 - val_loss: 3.0323 - val_acc: 0.4635\n",
      "Epoch 14/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4038 - acc: 0.5955 - val_loss: 3.2288 - val_acc: 0.4255\n",
      "Epoch 15/200\n",
      "390/390 [==============================] - 189s 485ms/step - loss: 2.4095 - acc: 0.5979 - val_loss: 3.6287 - val_acc: 0.4034\n",
      "Epoch 16/200\n",
      "390/390 [==============================] - 189s 485ms/step - loss: 2.4046 - acc: 0.6051 - val_loss: 2.9058 - val_acc: 0.5021\n",
      "Epoch 17/200\n",
      "390/390 [==============================] - 189s 485ms/step - loss: 2.4098 - acc: 0.6070 - val_loss: 3.5250 - val_acc: 0.3997\n",
      "Epoch 18/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4187 - acc: 0.6097 - val_loss: 2.9717 - val_acc: 0.4908\n",
      "Epoch 19/200\n",
      "390/390 [==============================] - 189s 485ms/step - loss: 2.4073 - acc: 0.6179 - val_loss: 3.1861 - val_acc: 0.4579\n",
      "Epoch 20/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4236 - acc: 0.6200 - val_loss: 3.2337 - val_acc: 0.4601\n",
      "Epoch 21/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4270 - acc: 0.6215 - val_loss: 2.9973 - val_acc: 0.4986\n",
      "Epoch 22/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4249 - acc: 0.6264 - val_loss: 3.0939 - val_acc: 0.4964\n",
      "Epoch 23/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4227 - acc: 0.6305 - val_loss: 3.2259 - val_acc: 0.4683\n",
      "Epoch 24/200\n",
      "390/390 [==============================] - 189s 483ms/step - loss: 2.4387 - acc: 0.6322 - val_loss: 3.1392 - val_acc: 0.4775\n",
      "Epoch 25/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4471 - acc: 0.6337 - val_loss: 3.1098 - val_acc: 0.5027\n",
      "Epoch 26/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4557 - acc: 0.6358 - val_loss: 3.4679 - val_acc: 0.4461\n",
      "Epoch 27/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4607 - acc: 0.6397 - val_loss: 3.5740 - val_acc: 0.4295\n",
      "Epoch 28/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4577 - acc: 0.6426 - val_loss: 3.5683 - val_acc: 0.4545\n",
      "Epoch 29/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4742 - acc: 0.6427 - val_loss: 3.0812 - val_acc: 0.5054\n",
      "Epoch 30/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4765 - acc: 0.6473 - val_loss: 3.7223 - val_acc: 0.3935\n",
      "Epoch 31/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4783 - acc: 0.6460 - val_loss: 2.9056 - val_acc: 0.5630\n",
      "Epoch 32/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4792 - acc: 0.6497 - val_loss: 3.1413 - val_acc: 0.5088\n",
      "Epoch 33/200\n",
      "390/390 [==============================] - 189s 483ms/step - loss: 2.4815 - acc: 0.6511 - val_loss: 2.9475 - val_acc: 0.5396\n",
      "Epoch 34/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.4959 - acc: 0.6506 - val_loss: 3.2823 - val_acc: 0.4939\n",
      "Epoch 35/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.4967 - acc: 0.6536 - val_loss: 3.0346 - val_acc: 0.5406\n",
      "Epoch 36/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5000 - acc: 0.6546 - val_loss: 3.4698 - val_acc: 0.4682\n",
      "Epoch 37/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5100 - acc: 0.6526 - val_loss: 3.0837 - val_acc: 0.5217\n",
      "Epoch 38/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 2.5112 - acc: 0.6581 - val_loss: 3.3968 - val_acc: 0.4782\n",
      "Epoch 39/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5164 - acc: 0.6598 - val_loss: 3.2999 - val_acc: 0.5126\n",
      "Epoch 40/200\n",
      "390/390 [==============================] - 189s 483ms/step - loss: 2.5322 - acc: 0.6551 - val_loss: 3.1625 - val_acc: 0.5191\n",
      "Epoch 41/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 2.5419 - acc: 0.6592 - val_loss: 3.0912 - val_acc: 0.5452\n",
      "Epoch 42/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5398 - acc: 0.6607 - val_loss: 3.2543 - val_acc: 0.5132\n",
      "Epoch 43/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5410 - acc: 0.6638 - val_loss: 3.0890 - val_acc: 0.5437\n",
      "Epoch 44/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5444 - acc: 0.6625 - val_loss: 3.5928 - val_acc: 0.4776\n",
      "Epoch 45/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 2.5567 - acc: 0.6634 - val_loss: 3.2593 - val_acc: 0.5164\n",
      "Epoch 46/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5539 - acc: 0.6663 - val_loss: 3.1630 - val_acc: 0.5272\n",
      "Epoch 47/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 2.5734 - acc: 0.6635 - val_loss: 3.1420 - val_acc: 0.5414\n",
      "Epoch 48/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5698 - acc: 0.6670 - val_loss: 3.4717 - val_acc: 0.5008\n",
      "Epoch 49/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5901 - acc: 0.6640 - val_loss: 3.1657 - val_acc: 0.5516\n",
      "Epoch 50/200\n",
      "390/390 [==============================] - 189s 484ms/step - loss: 2.5944 - acc: 0.6667 - val_loss: 3.2408 - val_acc: 0.5279\n",
      "Epoch 51/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5999 - acc: 0.6678 - val_loss: 3.2953 - val_acc: 0.5238\n",
      "Epoch 52/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 2.6051 - acc: 0.6711 - val_loss: 3.1707 - val_acc: 0.5446\n",
      "Epoch 53/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.5924 - acc: 0.6744 - val_loss: 3.6114 - val_acc: 0.4915\n",
      "Epoch 54/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.6292 - acc: 0.6676 - val_loss: 3.3179 - val_acc: 0.5302\n",
      "Epoch 55/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 2.6249 - acc: 0.6697 - val_loss: 3.2785 - val_acc: 0.5280\n",
      "Epoch 56/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.6356 - acc: 0.6697 - val_loss: 3.3748 - val_acc: 0.5167\n",
      "Epoch 57/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.6303 - acc: 0.6742 - val_loss: 3.5628 - val_acc: 0.4830\n",
      "Epoch 58/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.6311 - acc: 0.6708 - val_loss: 3.3933 - val_acc: 0.5207\n",
      "Epoch 59/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 2.6460 - acc: 0.6711 - val_loss: 3.3992 - val_acc: 0.5197\n",
      "Epoch 60/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.6433 - acc: 0.6697 - val_loss: 2.9264 - val_acc: 0.6040\n",
      "Epoch 61/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 2.1289 - acc: 0.7938 - val_loss: 2.2602 - val_acc: 0.7386\n",
      "Epoch 62/200\n",
      "390/390 [==============================] - 188s 483ms/step - loss: 1.7696 - acc: 0.8482 - val_loss: 2.0968 - val_acc: 0.7484\n",
      "Epoch 63/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 1.5782 - acc: 0.8684 - val_loss: 2.0068 - val_acc: 0.7461\n",
      "Epoch 64/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 1.4500 - acc: 0.8751 - val_loss: 1.9523 - val_acc: 0.7459\n",
      "Epoch 65/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.3553 - acc: 0.8818 - val_loss: 1.9634 - val_acc: 0.7327\n",
      "Epoch 66/200\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.3045 - acc: 0.8820 - val_loss: 1.9653 - val_acc: 0.7218\n",
      "Epoch 67/200\n",
      "390/390 [==============================] - 187s 481ms/step - loss: 1.2741 - acc: 0.8802 - val_loss: 1.9663 - val_acc: 0.7157\n",
      "Epoch 68/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2442 - acc: 0.8829 - val_loss: 1.9890 - val_acc: 0.7159\n",
      "Epoch 69/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2426 - acc: 0.8816 - val_loss: 1.9749 - val_acc: 0.7114\n",
      "Epoch 70/200\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.2336 - acc: 0.8845 - val_loss: 2.0856 - val_acc: 0.6939\n",
      "Epoch 71/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2388 - acc: 0.8824 - val_loss: 2.0215 - val_acc: 0.7102\n",
      "Epoch 72/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 1.2491 - acc: 0.8830 - val_loss: 2.0381 - val_acc: 0.7132\n",
      "Epoch 73/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2425 - acc: 0.8862 - val_loss: 2.0252 - val_acc: 0.7123\n",
      "Epoch 74/200\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.2383 - acc: 0.8904 - val_loss: 2.0680 - val_acc: 0.7021\n",
      "Epoch 75/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2498 - acc: 0.8898 - val_loss: 2.0829 - val_acc: 0.7039\n",
      "Epoch 76/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2406 - acc: 0.8937 - val_loss: 2.0621 - val_acc: 0.7067\n",
      "Epoch 77/200\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.2448 - acc: 0.8934 - val_loss: 2.1281 - val_acc: 0.7004\n",
      "Epoch 78/200\n",
      "390/390 [==============================] - 187s 479ms/step - loss: 1.2531 - acc: 0.8950 - val_loss: 2.0860 - val_acc: 0.7021\n",
      "Epoch 79/200\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.2479 - acc: 0.8991 - val_loss: 2.3784 - val_acc: 0.6683\n",
      "Epoch 80/200\n",
      "390/390 [==============================] - 187s 480ms/step - loss: 1.2671 - acc: 0.8958 - val_loss: 2.1517 - val_acc: 0.7019\n",
      "Epoch 81/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2664 - acc: 0.8989 - val_loss: 2.0318 - val_acc: 0.7191\n",
      "Epoch 82/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2593 - acc: 0.9028 - val_loss: 2.0789 - val_acc: 0.7155\n",
      "Epoch 83/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 1.2624 - acc: 0.9037 - val_loss: 2.1727 - val_acc: 0.6992\n",
      "Epoch 84/200\n",
      "390/390 [==============================] - 188s 482ms/step - loss: 1.2835 - acc: 0.8990 - val_loss: 2.2107 - val_acc: 0.7020\n",
      "Epoch 85/200\n",
      "390/390 [==============================] - 188s 481ms/step - loss: 1.2794 - acc: 0.9030 - val_loss: 2.2403 - val_acc: 0.7059\n",
      "Epoch 86/200\n",
      "131/390 [=========>....................] - ETA: 1:58 - loss: 1.2780 - acc: 0.9054"
     ]
    }
   ],
   "source": [
    "tasks = ['C10-CNN2','C100-WRN']\n",
    "optimizers = ['SGD','SGD_layca','SGD_weight_decay'] # also available: Adam, RMSProp, Adagrad\n",
    "\n",
    "for task in tasks:\n",
    "    x_train, y_train, x_test, y_test, get_model = import_task(task)\n",
    "    x_val, y_val = x_test, y_test\n",
    "    \n",
    "    # creates empty dictionary if first time the task is seen\n",
    "    if save_results:\n",
    "        results = load_results()\n",
    "        if task not in results.keys():\n",
    "            update_results([],{task:{}})\n",
    "    elif task not in results.keys():\n",
    "        results.update({task:{}})\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        \n",
    "        if save_results:\n",
    "            results = load_results()\n",
    "            if optimizer not in results[task].keys():\n",
    "                update_results([task],{optimizer:{'history':{'history':{'val_acc':[-1]}}}}) # save a bad initial performance\n",
    "        elif optimizer not in results[task].keys():\n",
    "            results[task].update({optimizer:{'history':{'history':{'val_acc':[-1]}}}})\n",
    "     \n",
    "        start = time.time()\n",
    "        # the weight decay parameter is taken from their original implementation and is specified in import_task.py (0.0005 for both tasks)\n",
    "        model = get_model(weight_decay = 0.) if 'weight_decay' not in optimizer else get_model()\n",
    "\n",
    "        batch_size = 128\n",
    "        # learning rate schedule is taken from their original implementation and is specified in get_training_utils.py\n",
    "        epochs, lr_scheduler = get_training_schedule_sot(task,optimizer)\n",
    "        verbose = 0\n",
    "\n",
    "        batch_frequency = int((x_train.shape[0]/batch_size))+5 # higher value than # of batches per epoch means once per epoch\n",
    "        ladc = LayerwiseAngleDeviationCurves(batch_frequency = batch_frequency)\n",
    "\n",
    "        callbacks = [lr_scheduler, ladc]\n",
    "    \n",
    "        # C100-WRN + SGD is the only case where nesterov momentum is used (cfr. original implementation)\n",
    "        if task == 'C100-WRN' and optimizer in ['SGD','SGD_weight_decay']: \n",
    "            opt = SGD(lr=0.1, momentum=0.9, nesterov=True) # lr is specified in lr_scheduler, not here\n",
    "        else:\n",
    "            opt = get_optimizer(optimizer, 0.1) # lr is specified in lr_scheduler, not here\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer= opt,\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        # data augmentation\n",
    "        datagen = ImageDataGenerator(width_shift_range=0.125,\n",
    "                     height_shift_range=0.125,\n",
    "                     fill_mode='reflect',\n",
    "                     horizontal_flip=True)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "            history = model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                                          steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                                          epochs = epochs,\n",
    "                                          verbose = verbose,\n",
    "                                          validation_data = (x_val, y_val),\n",
    "                                          callbacks = callbacks)\n",
    "\n",
    "        test_performance = model.evaluate(x_test,y_test, verbose = verbose)\n",
    "\n",
    "        if save_results:\n",
    "            update_results([task],{optimizer:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                              'test_performance':test_performance}})\n",
    "        else:\n",
    "            results[task].update({optimizer:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                             'test_performance':test_performance}})\n",
    "\n",
    "        with open(monitor_file,'a') as file:\n",
    "            file.write(task + ', '+optimizer+': done in '+str(time.time()-start)+' seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
