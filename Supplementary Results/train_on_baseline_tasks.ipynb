{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is copied from the eponym notebook in the parent folder, with additional options used for our analysis (e.g. storing the norm of weights and gradients during training.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sicarbonnell/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "\n",
    "import math as m\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from experiment_utils import history_todict, get_val_split\n",
    "from rotation_rate_utils import LayerwiseAngleDeviationCurves, get_kernel_layer_names\n",
    "from training_monitoring import trainingMemories, LayerRotationRateCurves, Adam_2nd_moment_memory\n",
    "\n",
    "from import_task import import_task\n",
    "from get_training_utils import get_training_schedule, get_stopping_criteria, get_optimizer, get_learning_rate_multipliers\n",
    "from LARS import LARS\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_results():\n",
    "    if not os.path.isfile('results.p'):\n",
    "        return {}\n",
    "    else:\n",
    "        with open('results.p','rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "def dump_results(results):\n",
    "    with open('results.p','wb') as f:\n",
    "        pickle.dump(dict(results),f)\n",
    "\n",
    "def update_results(path, new_data):\n",
    "    results = load_results()\n",
    "    position = results\n",
    "    for p in path:\n",
    "        position = position[p]\n",
    "    # new_data is a dictionary with the new (key,value) pairs\n",
    "    position.update(new_data)\n",
    "    dump_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_results = False\n",
    "if not save_results:\n",
    "    results = {}\n",
    "monitor_file = 'monitor_experiment.txt' \n",
    "\n",
    "# monitor_norms determines if the norms of the weights and weight gradients should be monitored during training and saved afterwards\n",
    "# it is only used for the SGD analysis\n",
    "monitor_norms = True\n",
    "monitor_layer_rotation_rates = True\n",
    "monitor_adam_moment = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tasks = ['C10-CNN1']\n",
    "optimizers = ['SGD', 'SGD_weight_decay'] # order is optimizer,_layca,_weight_decay\n",
    "alphas = [0.]#[-0.8, -0.6, -0.4, -0.3, -0.2, -0.1, 0., 0.1, 0.2, 0.3, 0.4, 0.6, 0.8]\n",
    "lrs = [3**-1]#3.**(-i) for i in range(-2,8)]\n",
    "\n",
    "for task in tasks:\n",
    "    x_train, y_train, x_test, y_test, get_model = import_task(task)\n",
    "    \n",
    "    # validation set is needed for early stopping or learning rate/alpha selection\n",
    "    [x_train, y_train], [x_val, y_val] = get_val_split(x_train,y_train, 0.1)\n",
    "    \n",
    "    # creates empty dictionary if first time the task is seen\n",
    "    if save_results:\n",
    "        results = load_results()\n",
    "        if task not in results.keys():\n",
    "            update_results([],{task:{}})\n",
    "    elif task not in results.keys():\n",
    "        results.update({task:{}})\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        \n",
    "        weight_decay = 0. if 'weight_decay' not in optimizer else 1e-3\n",
    "        \n",
    "        if save_results:\n",
    "            results = load_results()\n",
    "            if optimizer not in results[task].keys():\n",
    "                update_results([task],{optimizer:{'history':{'history':{'val_acc':[-1]}}}}) # save a bad initial performance\n",
    "        elif optimizer not in results[task].keys():\n",
    "            results[task].update({optimizer:{'history':{'history':{'val_acc':[-1]}}}})\n",
    "     \n",
    "        for alpha in alphas:\n",
    "            # layer-wise learning rate multipliers parametrized by alpha have not been implemented for the adaptive gradient methods\n",
    "            # changing alpha in this code will have no effect on their training\n",
    "            \n",
    "            if save_results:\n",
    "                results = load_results()\n",
    "                if alpha not in results[task][optimizer].keys():\n",
    "                    update_results([task,optimizer],{alpha:{}})\n",
    "            elif alpha not in results[task][optimizer].keys():\n",
    "                results[task][optimizer].update({alpha:{}})\n",
    "            \n",
    "            for lr in lrs:\n",
    "                start = time.time()\n",
    "                model = get_model(weight_decay)\n",
    "                \n",
    "                # we use smaller initial weights when using LARS, as the weights increase a lot during training\n",
    "                if optimizer == 'LARS':\n",
    "                    layer_names = get_kernel_layer_names(model)\n",
    "                    factor = 1/4.\n",
    "                    for l in layer_names:\n",
    "                        w = model.get_layer(l).get_weights()\n",
    "                        w[0] = factor*w[0]\n",
    "                        model.get_layer(l).set_weights(w)\n",
    "\n",
    "                batch_size = 128\n",
    "                epochs, lr_scheduler = get_training_schedule(task,lr)\n",
    "                stop_callback = get_stopping_criteria(task)\n",
    "                verbose = 0\n",
    "\n",
    "                batch_frequency = int((x_train.shape[0]/batch_size))+5 # once per epoch\n",
    "                ladc = LayerwiseAngleDeviationCurves(batch_frequency = batch_frequency)\n",
    "\n",
    "                callbacks = [lr_scheduler, ladc] #, stop_callback\n",
    "                if monitor_norms:\n",
    "                    sample_indices = np.random.choice(x_train.shape[0],min(x_train.shape[0],2000)) # selecting 2000 samples randomly\n",
    "                    norm_memory = trainingMemories(x_train[sample_indices],y_train[sample_indices],batch_frequency)\n",
    "                    callbacks.append(norm_memory)\n",
    "                if monitor_layer_rotation_rates:\n",
    "                    lrrc = LayerRotationRateCurves()\n",
    "                    callbacks.append(lrrc)\n",
    "                if monitor_adam_moment:\n",
    "                    adamMoment = Adam_2nd_moment_memory(batch_frequency = batch_frequency)\n",
    "                    callbacks.append(adamMoment)\n",
    "\n",
    "                multipliers = get_learning_rate_multipliers(model,alpha = alpha)\n",
    "                metrics = ['accuracy', 'top_k_categorical_accuracy'] if 'tiny' in task else ['accuracy']\n",
    "                opt = get_optimizer(optimizer, lr, multipliers) if optimizer != 'LARS' else LARS(model,lr,multipliers=multipliers)\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer= opt,\n",
    "                              metrics=metrics)\n",
    "\n",
    "                # cifar100 and tinyImagenet need early stopping\n",
    "                if 'C100' in task or 'tiny' in task:\n",
    "                    weights_file = 'saved_weights/best_weights_'+str(np.random.randint(1e6))+'.h5'\n",
    "                    callbacks += [ModelCheckpoint(weights_file, monitor='val_acc', save_best_only=True, save_weights_only = True)]\n",
    "\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\") # removes warning from keras for slow callback\n",
    "                    history = model.fit(x_train,y_train,\n",
    "                                        epochs = epochs,\n",
    "                                        batch_size = batch_size,\n",
    "                                        verbose = verbose,\n",
    "                                        validation_data = (x_val, y_val),\n",
    "                                        callbacks = callbacks)\n",
    "\n",
    "                if 'C100' in task or 'tiny' in task:\n",
    "                    model.load_weights(weights_file)\n",
    "\n",
    "                test_performance = model.evaluate(x_test,y_test, verbose = verbose)\n",
    "\n",
    "                if save_results:\n",
    "                    update_results([task, optimizer,alpha],{lr:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                                'test_performance':test_performance}})\n",
    "                else:\n",
    "                    results[task][optimizer][alpha].update({lr:{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                                'test_performance':test_performance}})\n",
    "                \n",
    "                # if it beats current best validation performance of (task,optimizer) pair\n",
    "                if save_results:\n",
    "                    results = load_results()\n",
    "                if max(history.history['val_acc']) > max(results[task][optimizer]['history']['history']['val_acc']):\n",
    "                    if save_results:\n",
    "                        update_results([task,optimizer],{'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                         'test_performance':test_performance,'best_alpha':alpha,'best_lr':lr})\n",
    "                    else:\n",
    "                        results[task][optimizer].update({'history':history_todict(history),'ladc':ladc.memory,\n",
    "                                                         'test_performance':test_performance,'best_alpha':alpha,'best_lr':lr})\n",
    "                    \n",
    "                if monitor_norms:\n",
    "                    with open('results/norm_memory_'+task+'_'+optimizer+'.p','wb') as f:\n",
    "                        pickle.dump(norm_memory.memory,f)\n",
    "                if monitor_layer_rotation_rates:\n",
    "                    with open('results/layer_rotation_rates_'+task+'_'+optimizer+'.p','wb') as f:\n",
    "                        pickle.dump(lrrc.memory,f)\n",
    "                if monitor_adam_moment:\n",
    "                    with open('results/Adam_Moments_'+task+'.p','wb') as f:\n",
    "                        pickle.dump(adamMoment.memory,f)\n",
    "                    \n",
    "#                 with open(monitor_file,'a') as file:\n",
    "#                     file.write(task + ', '+optimizer+', '+str(alpha)+ ', '+str(lr)+': done in '+str(time.time()-start)+' seconds.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
